Textual embeddings are a way to represent text in a numerical format that models semantic and syntactic meaning. 

Each embedding type has unique strengths and is chosen based on the specific requirements of the application, such as context sensitivity, domain-specificity, or granularity of text.

Below is a categorized list of various types of textual embeddings.

---

### **1. Granularity-Based Embeddings**

- **Character Embeddings**: Represent text at the character level. Useful for handling misspellings or unknown words. Example: CharCNN.
- **Subword Embeddings**: Focus on subword units (e.g., byte-pair encoding). Example: FastText.
- **Word Embeddings**: Represent individual words. Examples: Word2Vec, GloVe, FastText.
- **Phrase Embeddings**: Capture meaning for multi-word phrases (e.g., noun phrases or idioms).
- **Sentence Embeddings**: Represent entire sentences. Examples: Sentence-BERT, InferSent, Universal Sentence Encoder.
- **Paragraph Embeddings**: Represent a paragraph or a group of sentences. Example: Doc2Vec.
- **Document Embeddings**: Represent entire documents. Often constructed by aggregating word or sentence embeddings. Examples: Topic2Vec, Hierarchical embeddings.

---

### **2. Temporal-Based Embeddings**

- **Static Embeddings**: Pre-trained embeddings where the representation of a word is fixed regardless of context. Examples: Word2Vec, GloVe.
- **Contextual Embeddings**: Word representations change based on context within the sentence. Examples: ELMo, BERT, GPT, T5.

---

### **3. Semantic and Domain-Specific Embeddings**

- **Knowledge Graph Embeddings**: Represent entities and relations in a knowledge graph. Examples: TransE, RotatE.
- **Multimodal Embeddings**: Combine text with other modalities, such as images or audio. Example: CLIP (for image-text embeddings).
- **Domain-Specific Embeddings**: Tailored embeddings for specific domains like biomedicine (e.g., BioBERT) or finance (e.g., FinBERT).

---

### **4. Multi-Language and Cross-Language Embeddings**

- **Monolingual Embeddings**: Trained for a single language.
- **Cross-Lingual Embeddings**: Represent text from multiple languages in a shared embedding space. Examples: MUSE, LASER.
- **Multilingual Embeddings**: Designed to handle multiple languages without explicitly mapping them to a shared space. Example: mBERT.

---

### **5. Compositional Embeddings**

- **Bag of Words (BoW)**: Basic embedding using word counts or frequencies.
- **TF-IDF Embeddings**: Reflect the importance of a word in a document relative to a corpus.
- **Embedding Aggregations**: Combine word embeddings using mean, max, or attention mechanisms to create sentence/paragraph/document embeddings.

---

### **6. Specialized Embeddings**

- **Graph-Based Embeddings**: Represent text in terms of graph relationships, such as TextGCN.
- **Hierarchical Embeddings**: Capture structure across sentences, paragraphs, and documents hierarchically.
- **Hyperbolic Embeddings**: Model hierarchical or tree-like relationships (e.g., Poincar√© embeddings).

---

### **7. Context-Adaptable Embeddings**

- **Dynamic Embeddings**: Generate on-the-fly based on new data or user context.
- **Prompt-Based Embeddings**: Use prompts to generate embeddings for specific tasks. Example: OpenAI GPT models using instruction-tuned embeddings.

---

### **8. Task-Specific Embeddings**

- **Classification-Oriented Embeddings**: Fine-tuned for text classification tasks.
- **Sequence-to-Sequence Embeddings**: Designed for tasks like translation or summarization. Example: Seq2Seq models like T5.
- **Similarity Embeddings**: Optimized for similarity tasks such as semantic search or clustering.
