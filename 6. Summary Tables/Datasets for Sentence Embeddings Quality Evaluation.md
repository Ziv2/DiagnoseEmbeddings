# Literature Review of Methods to Evaluate Text Embeddings

## Datasets for Sentence Embeddings Quality Evaluation

| Name of Dataset                        | Link to the Dataset                                                                                  | License                      | Relevant Models        | How to Use                                                                     | Ease of Implementation | Importance                                                      |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------- | ---------------------- | ------------------------------------------------------------------------------ | ---------------------- | --------------------------------------------------------------- |
| STS Benchmark                          | [STS Benchmark](http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark)                               | Free for research purposes   | SBERT, USE, InferSent  | Evaluate semantic textual similarity with human-annotated sentence pairs.      | Easy                   | High - gold standard for sentence similarity evaluation.        |
| SICK Dataset                           | [SICK Dataset](https://github.com/alvations/SICK)                                                    | Creative Commons Attribution | SBERT, InferSent       | Test sentence similarity and entailment capabilities of embeddings.            | Moderate               | High - evaluates both similarity and entailment relationships.  |
| Quora Question Pairs                   | [Quora Question Pairs](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs) | Quora-specific license       | USE, SBERT             | Measure embedding performance on duplicate question detection.                 | Moderate               | High - real-world relevance for question-matching tasks.        |
| STS12-16 (Semantic Textual Similarity) | [STS12-16](http://ixa2.si.ehu.eus/stswiki/)                                                          | Free for research purposes   | SBERT, USE             | Assess semantic similarity with diverse sentence pairs from multiple datasets. | Easy                   | High - benchmark for cross-year sentence similarity evaluation. |
| Multi-Genre NLI (MNLI) Corpus          | [MNLI](https://cims.nyu.edu/~sbowman/multinli/)                                                      | Free for research purposes   | BERT, RoBERTa, DeBERTa | Evaluate entailment and contradiction in sentence embeddings.                  | Moderate               | High - provides multi-domain entailment testing.                |
